<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Adaptive Sensing | Sensus Documentation </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Adaptive Sensing | Sensus Documentation ">
    <meta name="generator" content="docfx 2.38.1.0">
    
    <link rel="shortcut icon" href="../images/favicon.ico">
    <link rel="stylesheet" href="../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../styles/docfx.css">
    <link rel="stylesheet" href="../styles/main.css">
    <meta property="docfx:navrel" content="../toc.html">
    <meta property="docfx:tocrel" content="toc.html">
    
    <meta property="docfx:rel" content="../">
    
  </head>
  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../index.html">
                <img id="logo" class="svg" src="../images/group-of-members-users-icon.png" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div class="container body-content">
        
        <div id="search-results">
          <div class="search-list"></div>
          <div class="sr-items">
            <p><i class="glyphicon glyphicon-refresh index-loading"></i></p>
          </div>
          <ul id="pagination"></ul>
        </div>
      </div>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="adaptive_sensing">
<h1 id="adaptive-sensing">Adaptive Sensing</h1>

<p>An essential trade off within all mobile sensing apps is between (a) data density (high sampling
rates) and continuity (no data gaps) and (b) battery drain. It is easy to configure Sensus to 
optimize either of these objectives, but striking a balance between them poses significant 
challenges. For example, one might wish to sample high-density accelerometry when the device
is likely to be used in particular ways (e.g., while walking). One might wish to sample light 
and temperature data in particular geographic locations. No single, static sensing configuration 
would satisfy such objectives. Rather, dynamic sensing configurations that adapt to the context
of use are required. This article describes the way in which Sensus supports adaptive sensing.</p>
<h2 id="adaptive-control">Adaptive Control</h2>
<p>The figure below depicts the adaptive sensing state diagram.</p>
<p><img src="/sensus/images/adaptive-sensing-state-diagram.png" alt="image"></p>
<p>The following key concepts are indicated:</p>
<ul>
<li><p>Opportunistic observation:  Sensing agents can be configured to observe state information (i.e.,
data) actively or opportunistically. Opportunistic observation will capture any 
<a class="xref" href="../api/Sensus.IDatum.html">IDatum</a> object generated by Sensus during normal operation. In this mode, the app
will not take any extra actions to observe data. This has the effect of reducing battery drain at 
the expense of weaker state estimates and sensing control.</p>
</li>
<li><p>Opportunistic control:  In response to opportunistically observed data, the sensing agent
may decide to control sensing in a particular way (e.g., by enabling continuous sensing or
increasing sampling rates). Such decisions will only be possible upon the arrival of 
opportunistic data.</p>
</li>
<li><p>Action interval:  In contrast with opportunistic sensing, sensing agents can be configured to 
take extra actions to observe data. This has the effect of strengthening state estimates and sensing 
control at the expense of increased battery drain. The action interval indicates how frequently Sensus 
should actively observe data for state estimation.</p>
</li>
<li><p>Active observation duration:  Once the action interval elapses, Sensus will begin to actively
observe each <a class="xref" href="../api/Sensus.IDatum.html">IDatum</a> generated by the app. This parameter governs how long the
observation should continue before checking the control criterion.</p>
</li>
<li><p>Control criterion:  Regardless of whether Sensus is observing opportunistically or actively, 
the control criterion defines state estimate values that trigger sensing control. For example,
a control criterion might specify that sensing control should occur when the average acceleration 
magnitude exceeds a critical threshold. This is demonstrated in the 
<a href="https://github.com/predictive-technology-laboratory/sensus/blob/develop/ExampleSensingAgent.Shared/ExampleAccelerationSensingAgent.cs">example</a>
sensing agent.</p>
</li>
<li><p>Control completion check interval:  Once sensing control is invoked, the app will periodically 
recheck the control criterion to determine whether or not it is still met. If it is not, then
sensing control ends and the sensing agent returns to its idle state. If it is, then sensing 
control continues until the next control completion check occurs. This parameter governs how
long Sensus should wait between each completion check.</p>
</li>
</ul>
<h2 id="android">Android</h2>
<h3 id="sensing-agent-plug-ins">Sensing Agent Plug-Ins</h3>
<p>On Android, Sensus supports a plug-in architecture for modules (or agents) that control Sensing configuration.
This architecture is intended to support research into adaptive sensing by providing a simple interface
through which researchers can deploy agents that implement specific adaptation approaches.</p>
<h3 id="implementing-and-deploying-a-sensing-agent-plug-in">Implementing and Deploying a Sensing Agent Plug-In</h3>
<p>Follow the steps below to implement and deploy a sensing agent within your Sensus study.</p>
<ol>
<li>Create a new Android Class Library project in Visual Studio. In Visual Studio for Mac, the following image
shows the correct selection:</li>
</ol>
<p><img src="/sensus/images/survey-agent-project.png" alt="image"></p>
<ol>
<li><p>Add a NuGet reference to <a href="https://www.nuget.org/packages/Sensus">the Sensus package</a>.</p>
</li>
<li><p>Add a new class that inherits from <a class="xref" href="../api/Sensus.SensingAgent.html">SensingAgent</a>. Implement all abstract methods.</p>
</li>
<li><p>Build the library project, and upload the resulting .dll to a web-accessible URL. A convenient
solution is to upload the .dll to a Dropbox directory and copy the sharing URL for the .dll file.</p>
</li>
<li><p>Generate a QR code that points to your .dll (e.g., using <a href="https://www.qr-code-generator.com/">QR Code Generator</a>).
The content of the QR code must be exactly as shown below:</p>
<pre><code>sensing-agent:URL
</code></pre><p>where URL is the web-accessible URL that points to your .dll file. If you are using Dropbox, then the QR code
content will be similar to the one shown below (note the <code>dl=1</code> ending of the URL, and note that the following 
URL is only an example -- it is not actually valid):</p>
<pre><code>sensing-agent:https://www.dropbox.com/s/dlaksdjfasfasdf/SensingAgent.dll?dl=1
</code></pre></li>
<li><p>In your <a class="xref" href="../api/Sensus.Protocol.html">Protocol</a> settings, tap &quot;Set Agent&quot; and scan your QR code. Sensus will fetch your .dll file and 
extract any agent definitions contained therein. Select your desired agent.</p>
</li>
<li><p>Continue with <a class="xref" href="protocol-creation.html">configuration</a> and <a class="xref" href="protocol-distribution.html">distribution</a>
of your protocol.</p>
</li>
</ol>
<h3 id="example-sensing-agents">Example Sensing Agents</h3>
<p>See the following implementations for example agents:</p>
<ul>
<li><a class="xref" href="../api/ExampleSensingAgent.ExampleAccelerationSensingAgent.html">Acceleration</a> (code <a href="https://github.com/predictive-technology-laboratory/sensus/blob/develop/ExampleSensingAgent.Shared/ExampleAccelerationSensingAgent.cs">here</a>):  A 
sensing agent that samples continuously if the device is moving or near a surface (e.g., face).</li>
</ul>
<h2 id="ios">iOS</h2>
<p>In contrast with Android, iOS does not allow apps to load code (e.g., from the above .dll assembly) at
run time. Thus, adaptive sensing agents are more limited on iOS compared with Android. Here are the options:</p>
<ul>
<li><p>The app comes with one example sensing agent; however, it is simply for demonstration and is unlikely to work
well in practice. Nonetheless, the examples is:</p>
<ul>
<li><a class="xref" href="../api/ExampleSensingAgent.ExampleAccelerationSensingAgent.html">Acceleration</a> (code <a href="https://github.com/predictive-technology-laboratory/sensus/blob/develop/ExampleSensingAgent.Shared/ExampleAccelerationSensingAgent.cs">here</a>):  A 
sensing agent that samples continuously if the device is moving or near a surface (e.g., face).</li>
</ul>
</li>
</ul>
<p>You can select this agent when configuring the <a class="xref" href="../api/Sensus.Protocol.html">Protocol</a>.</p>
<ul>
<li><p>You can <a class="xref" href="redeploying-sensus.html">redeploy</a> Sensus as your own app, to which you can add your own agent implementations.</p>
</li>
<li><p>You can implement your own agent implementations following the instructions above for Android and email 
our team (uva.ptl@gmail.com) to include them in a future release.</p>
</li>
</ul>
<h2 id="testing-and-debugging">Testing and Debugging</h2>
<p>Regardless of whether your sensing agent targets Android or iOS, there are a few ways to test and debug it:</p>
<ul>
<li><p>Monitor the agent state:  Within your <a class="xref" href="../api/Sensus.Protocol.html">Protocol</a> settings, tap &quot;View Agent State&quot; to see a real-time
display of your agent&#39;s state. You will see it cycle through the state diagram shown above.</p>
</li>
<li><p>Write to the log file:  See the code for the example agents above. You will see calls that write to the log file. Use similar
calls in your code to write information about the behavior of your agent to the log. Run your agent for a while in the app and
then share the log file with yourself from within the app. Note that the size of the log file is limited, so you might not be 
able to view the entire log history of your agent.</p>
</li>
<li><p>Flash notifications on the screen:  On Android, you can flash notifications on the screen as shown in the example code. These
messages will appear for a short duration.</p>
</li>
<li><p>Run your agent in the debugger:  By far the most useful approach is to <a class="xref" href="configuring-a-development-system.html">configure a development system</a> and
run Sensus in the debugger with your sensing agent. You will need to add your agent code to the Sensus app projects in order to 
step through it in the debugger.</p>
</li>
</ul>
</article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                  <li>
                    <a href="https://github.com/predictive-technology-laboratory/sensus/blob/develop/DocFX/articles/adaptive-sensing.md/#L1" class="contribution-link">Improve this Doc</a>
                  </li>
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            Copyright © 2014-2018 University of Virginia<br>Generated by <strong>DocFX</strong>
            
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../styles/docfx.js"></script>
    <script type="text/javascript" src="../styles/main.js"></script>
  </body>
</html>
